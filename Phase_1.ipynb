{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr3U-FJeUoi-"
      },
      "outputs": [],
      "source": [
        "#importing necessary Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from os import walk\n",
        "np.random.seed(1525)\n",
        "#setting size of images to be sent into Hopfield\n",
        "WE = 32\n",
        "HE = 32\n",
        "target_size=(WE,HE)\n",
        "\n",
        "#Function to add noise to test images\n",
        "def get_corrupted_input(input, corruption_level):\n",
        "    corrupted = np.copy(input)\n",
        "    inv = np.random.binomial(n=1, p=corruption_level, size=len(input))\n",
        "    for i, v in enumerate(input):\n",
        "        if inv[i]:\n",
        "            corrupted[i] = 0 * v\n",
        "    return corrupted\n",
        "\n",
        "#Load our images from given file paths, resize and flatten them to use in Hopfield\n",
        "def load_and_preprocess_images(image_paths, target_size):\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        image = Image.open(path)  # Open our images\n",
        "        image = image.resize(target_size)\n",
        "        image_array = np.array(image) / 255.0  # Normalize pixel values (0-1)\n",
        "        images.append(image_array.flatten())  # Flatten the 2D array to 1D\n",
        "    return images\n",
        "\n",
        "#Loading training image paths all at once\n",
        "res = []\n",
        "fnames = None\n",
        "for (dir_path, dir_names, file_names) in walk(\"/content/drive/MyDrive/train_imgs\"):\n",
        "  fnames = file_names\n",
        "\n",
        "for name in fnames:\n",
        "  res.append(\"/content/drive/MyDrive/train_imgs/\" + name)\n",
        "print(\"Images loaded for training : \", len(res))\n",
        "\n",
        "\n",
        "#Loading test image paths and their relavant data like which category and how many examplars into a list\n",
        "from os import walk\n",
        "dir_main = \"/content/drive/MyDrive/Gray2\"\n",
        "dir_path_list = []\n",
        "for (dir_path, dir_names, file_names) in walk(dir_main):\n",
        "    dir_path_list.append(dir_path)\n",
        "dir_path_list.remove(\"/content/drive/MyDrive/Gray2\")\n",
        "dir_path_list.remove(\"/content/drive/MyDrive/Gray2/1exemplar\")\n",
        "dir_path_list.remove(\"/content/drive/MyDrive/Gray2/16exemplars\")\n",
        "dir_path_list.remove(\"/content/drive/MyDrive/Gray2/8exemplars\")\n",
        "dir_path_list.remove(\"/content/drive/MyDrive/Gray2/4exemplars\")\n",
        "dir_path_list.remove(\"/content/drive/MyDrive/Gray2/2exemplars\")\n",
        "test_img_info = []\n",
        "dpl = \"/content/drive/MyDrive/Gray2/1exemplar\"\n",
        "fnames = []\n",
        "for (dir_path, dir_names, file_names) in walk(dpl):\n",
        "  fnames = file_names\n",
        "for file_names in fnames:\n",
        "  test_img_info.append(['1exemplar', file_names, dpl +\"/\"+ file_names])\n",
        "for dpl in dir_path_list:\n",
        "  for (dir_path, dir_names, file_names) in walk(dpl):\n",
        "    test_img_info.append([dir_path.split('/')[-2],dir_path.split('/')[-1], dir_path +\"/\"+ file_names[1]])\n",
        "test_img_paths = []\n",
        "for info in test_img_info:\n",
        "  test_img_paths.append(info[2])\n",
        "print(\"Images taken for testing : \", len(test_img_paths))\n",
        "\n",
        "\n",
        "\n",
        "#Calling funtions to get train and test images\n",
        "image_paths = res\n",
        "vectorized_images = load_and_preprocess_images(image_paths, target_size)\n",
        "random.shuffle(vectorized_images)\n",
        "test_images = load_and_preprocess_images(test_img_paths, target_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of neurons in the network\n",
        "num_neurons = WE * HE  # 32x32 images\n",
        "\n",
        "# Number of patterns to store\n",
        "num_patterns = len(vectorized_images)\n",
        "\n",
        "# Size of each pattern (32x32 images)\n",
        "pattern_size = WE * HE\n",
        "\n",
        "# Generate random grayscale patterns for training\n",
        "patterns = np.array(vectorized_images)\n",
        "\n",
        "# Weight matrix initialization\n",
        "weights = np.zeros((num_neurons, num_neurons))\n",
        "\n",
        "# Hebbian learning rule to train the network\n",
        "for pattern in patterns:\n",
        "    pattern = pattern.reshape(num_neurons, 1)  # Reshape the pattern to a column vector\n",
        "    weights += np.outer(pattern, pattern)\n",
        "\n",
        "# Set diagonal elements to 0 (no self-connections)\n",
        "np.fill_diagonal(weights, 0)\n",
        "\n",
        "# Update function for continuous Hopfield network (asynchronous update)\n",
        "def update_hopfield_network(input_pattern, weights, num_steps=100):\n",
        "    pattern = input_pattern.copy()\n",
        "    for _ in range(num_steps):\n",
        "        neuron_idx = np.random.randint(0, num_neurons)  # Randomly select a neuron\n",
        "        activation = np.dot(weights[neuron_idx], pattern)\n",
        "        pattern[neuron_idx] = np.tanh(activation)  # Apply the activation function\n",
        "    return pattern\n",
        "\n",
        "# Function to calculate the similarity between two patterns\n",
        "def calculate_similarity(pattern1, pattern2):\n",
        "    return np.dot(pattern1, pattern2)\n",
        "\n",
        "# Test the network by retrieving patterns\n",
        "retrieved_patterns = []\n",
        "similarities = []\n",
        "\n",
        "patterns2 = np.array([get_corrupted_input(d, 0.1) for d in test_images])\n",
        "p3 = np.array(test_images)\n",
        "\n",
        "for i in range(patterns2.shape[0]):\n",
        "    retrieved_pattern = update_hopfield_network(patterns2[i], weights)\n",
        "    retrieved_patterns.append(retrieved_pattern)\n",
        "    similarity = calculate_similarity(p3[i], retrieved_pattern)\n",
        "    test_img_info[i].append(similarity * 100)\n",
        "    similarities.append(similarity)"
      ],
      "metadata": {
        "id": "BpA7tQWWUxiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing results in an excel sheet\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(test_img_info, columns=['Exemplar', 'Category', 'Path', 'Percent Correct'])\n",
        "df.to_excel('/content/drive/MyDrive/crcT222.xlsx')"
      ],
      "metadata": {
        "id": "kcY_OzXoVOPw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}